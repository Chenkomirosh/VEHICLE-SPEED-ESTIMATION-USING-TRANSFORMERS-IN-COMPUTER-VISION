{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kosty\\anaconda3\\lib\\site-packages (2.5.1+cu118)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\kosty\\anaconda3\\lib\\site-packages (0.20.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\kosty\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: supervision in c:\\users\\kosty\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kosty\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kosty\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: filelock in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.7 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from supervision) (1.2.0)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from supervision) (3.9.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from supervision) (1.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kosty\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers supervision tqdm opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vehicles.mp4 assets \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee2a44d6de44e63b76cecfc1473dc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35345757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n",
      "100%|██████████| 538/538 [26:03<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import supervision as sv\n",
    "\n",
    "# Download assets for video processing\n",
    "from supervision.assets import VideoAssets, download_assets\n",
    "download_assets(VideoAssets.VEHICLES)\n",
    "\n",
    "# File paths and constants\n",
    "SOURCE_VIDEO_PATH = \"vehicles.mp4\"\n",
    "TARGET_VIDEO_PATH = \"vehicles-result.mp4\"\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "IOU_THRESHOLD = 0.5\n",
    "MODEL_RESOLUTION = 1280\n",
    "SOURCE = np.array([\n",
    "    [1252, 787],\n",
    "    [2298, 803],\n",
    "    [5039, 2159],\n",
    "    [-550, 2159]\n",
    "])\n",
    "\n",
    "TARGET_WIDTH = 25\n",
    "TARGET_HEIGHT = 250\n",
    "TARGET = np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "])\n",
    "\n",
    "# Load the Swin Transformer model for object detection\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model.eval()\n",
    "\n",
    "# Initialize video processing\n",
    "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Annotation and tracking setup\n",
    "thickness = sv.calculate_optimal_line_thickness(resolution_wh=video_info.resolution_wh)\n",
    "text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    text_scale=text_scale,\n",
    "    text_thickness=thickness,\n",
    "    text_position=sv.Position.BOTTOM_CENTER\n",
    ")\n",
    "trace_annotator = sv.TraceAnnotator(\n",
    "    thickness=thickness,\n",
    "    trace_length=video_info.fps * 2,\n",
    "    position=sv.Position.BOTTOM_CENTER\n",
    ")\n",
    "\n",
    "polygon_zone = sv.PolygonZone(polygon=SOURCE)\n",
    "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n",
    "\n",
    "# Perspective transformation setup\n",
    "class ViewTransformer:\n",
    "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "        source = source.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "        if points.size == 0:\n",
    "            return points\n",
    "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
    "        return transformed_points.reshape(-1, 2)\n",
    "\n",
    "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\n",
    "\n",
    "# Process video and perform object detection\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
    "        # Convert frame to PIL image for processing with Swin\n",
    "        pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Preprocess frame and perform inference\n",
    "        inputs = processor(images=pil_frame, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Convert outputs to detection format\n",
    "        logits = outputs.logits[0]\n",
    "        boxes = outputs.pred_boxes[0]\n",
    "        scores = torch.softmax(logits, dim=-1)[:, :-1].max(dim=1).values\n",
    "        labels = torch.argmax(logits[:, :-1], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/538 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process video and perform object detection\n",
    "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
    "        # Convert frame to PIL image for processing with Swin\n",
    "        pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Preprocess frame and perform inference\n",
    "        inputs = processor(images=pil_frame, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract outputs and convert them into detections\n",
    "        logits = outputs.logits[0]\n",
    "        boxes = outputs.pred_boxes[0]\n",
    "        scores = torch.softmax(logits, dim=-1)[:, :-1].max(dim=1).values\n",
    "        labels = torch.argmax(logits[:, :-1], dim=1)\n",
    "\n",
    "        # Collect detections\n",
    "        detections = []\n",
    "        for score, label, box in zip(scores, labels, boxes):\n",
    "            if score > CONFIDENCE_THRESHOLD and label != 0:  # Filter detections\n",
    "                x_min, y_min, x_max, y_max = box.tolist()\n",
    "                detections.append(sv.Detection(\n",
    "                    x_min=int(x_min * frame.shape[1]),\n",
    "                    y_min=int(y_min * frame.shape[0]),\n",
    "                    x_max=int(x_max * frame.shape[1]),\n",
    "                    y_max=int(y_max * frame.shape[0]),\n",
    "                    confidence=float(score),\n",
    "                    class_id=int(label)\n",
    "                ))\n",
    "\n",
    "        sv_detections = sv.Detections(detections=detections)\n",
    "\n",
    "        # Filter detections inside the polygon zone\n",
    "        sv_detections = sv_detections[polygon_zone.trigger(sv_detections)]\n",
    "\n",
    "        # Pass detections through the tracker\n",
    "        byte_track = sv.ByteTrack(frame_rate=video_info.fps)\n",
    "        tracked_detections = byte_track.update_with_detections(detections=sv_detections)\n",
    "\n",
    "        # Calculate the detections position inside the target RoI\n",
    "        points = tracked_detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "        points = view_transformer.transform_points(points=points).astype(int)\n",
    "\n",
    "        # Store detections position for speed calculation\n",
    "        for tracker_id, [_, y] in zip(tracked_detections.tracker_id, points):\n",
    "            coordinates[tracker_id].append(y)\n",
    "\n",
    "        # Format labels and calculate speeds\n",
    "        labels = []\n",
    "        for tracker_id in tracked_detections.tracker_id:\n",
    "            if len(coordinates[tracker_id]) < video_info.fps / 2:\n",
    "                labels.append(f\"#{tracker_id}\")\n",
    "            else:\n",
    "                # Calculate speed\n",
    "                coordinate_start = coordinates[tracker_id][-1]\n",
    "                coordinate_end = coordinates[tracker_id][0]\n",
    "                distance = abs(coordinate_start - coordinate_end)\n",
    "                time = len(coordinates[tracker_id]) / video_info.fps\n",
    "                speed = distance / time * 3.6  # Convert to km/h\n",
    "                labels.append(f\"#{tracker_id} {int(speed)} km/h\")\n",
    "\n",
    "        # Annotate frame\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = trace_annotator.annotate(\n",
    "            scene=annotated_frame, detections=tracked_detections\n",
    "        )\n",
    "        annotated_frame = bounding_box_annotator.annotate(\n",
    "            scene=annotated_frame, detections=tracked_detections\n",
    "        )\n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame, detections=tracked_detections, labels=labels\n",
    "        )\n",
    "\n",
    "        # Write annotated frame to output video\n",
    "        sink.write_frame(annotated_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
